""""move_resources_to_mongo"

Revision ID: 577e258bd4fb
Revises: 636e5298d75a
Create Date: 2020-10-13 06:52:41.423173

"""
import os
import json
import uuid
import base64
from datetime import datetime
from pymongo import MongoClient
from sqlalchemy.exc import ProgrammingError
import sqlalchemy as sa
from alembic import op
from sqlalchemy.orm import Session
from sqlalchemy.ext.declarative import declarative_base, declared_attr
from sqlalchemy.sql import table, column, func
from config_client.client import Client as EtcdClient

# revision identifiers, used by Alembic.
revision = '577e258bd4fb'
down_revision = '636e5298d75a'
branch_labels = None
depends_on = None
DEFAULT_ETCD_HOST = 'etcd-client'
DEFAULT_ETCD_PORT = 80
CHUNK_SIZE = 2000


class Base(object):
    @declared_attr
    # pylint: disable=E0213
    def __tablename__(cls):
        # pylint: disable=E1101
        return cls.__name__.lower()

    def to_dict(self):
        return {c.key: getattr(self, c.key)
                for c in sa.inspect(self).mapper.column_attrs}


Base = declarative_base(cls=Base)


def gen_id():
    return str(uuid.uuid4())


def now_timestamp():
    return int(datetime.utcnow().timestamp())


def _get_etcd_config_client():
    etcd_host = os.environ.get('HX_ETCD_HOST', DEFAULT_ETCD_HOST)
    etcd_port = os.environ.get('HX_ETCD_PORT', DEFAULT_ETCD_PORT)
    config_cl = EtcdClient(host=etcd_host, port=int(etcd_port))
    return config_cl


def _get_resources_collection():
    config_cl = _get_etcd_config_client()
    mongo_params = config_cl.mongo_params()
    mongo_conn_string = "mongodb://%s:%s@%s:%s" % mongo_params[:-1]
    mongo_client = MongoClient(mongo_conn_string)
    return mongo_client.restapi.resources


class Resource(Base):
    id = sa.Column(sa.String(36), primary_key=True, default=gen_id)
    deleted_at = sa.Column(sa.Integer, default=0, nullable=False)
    created_at = sa.Column(sa.Integer, default=now_timestamp, nullable=False)

    cloud_resource_id = sa.Column(sa.String(36), nullable=False)
    name = sa.Column(sa.String(256), nullable=True)
    region = sa.Column(sa.String(256), nullable=True)
    employee_id = sa.Column(sa.String(36), nullable=True)
    cloud_account_id = sa.Column(sa.String(36), nullable=False)
    budget_id = sa.Column(sa.String(36), nullable=True)
    resource_type = sa.Column(sa.String(256), nullable=False)
    meta = sa.Column(sa.TEXT(), nullable=True, default='{}')


class Tag(Base):
    id = sa.Column(sa.String(36), default=gen_id, primary_key=True)
    resource_id = sa.Column(sa.String(36), nullable=False)
    key = sa.Column(sa.String(256), nullable=False)
    value = sa.Column(sa.String(256), nullable=False)


def _create_unique_index(collection):
    collection.create_index(
        [
            ('cloud_resource_id', 1), ('deleted_at', 1),
            ('cloud_account_id', 1)
        ],
        name='OptResourceUnique',
        unique=True
    )


def _create_search_indexes(collection):
    search_indexes = {
        'CloudAccountID': 'cloud_account_id',
        'CloudResourceID': 'cloud_resource_id',
        'BudgetID': 'budget_id',
        'EmlpoyeeID': 'employee_id',
        'LastSeen': 'last_seen'
    }
    for index_name, field_name in search_indexes.items():
        collection.create_index([(field_name, 1)], name=index_name)


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    bind = op.get_bind()
    session = Session(bind=bind)
    resources_collection = _get_resources_collection()

    try:
        rows = session.query(func.count(Resource.id)).scalar()
        migrated_cnt = 0
        while migrated_cnt < rows:
            resources = session.query(
                Resource
            ).limit(CHUNK_SIZE).offset(migrated_cnt).all()
            chunk = {}
            for r in resources:
                resource_dict = r.to_dict()
                meta = resource_dict.pop('meta', None) or '{}'
                resource_id = resource_dict.pop('id')
                try:
                    meta_object = json.loads(meta)
                except json.decoder.JSONDecodeError:
                    meta_object = {}
                resource_dict.update({
                    '_id': resource_id,
                    'meta': meta_object,
                    'last_seen': 0,
                    'tags': {}
                })
                chunk[resource_id] = resource_dict
            tags = session.query(Tag).filter(
                Tag.resource_id.in_(chunk.keys())
            ).all()
            for tag in tags:
                tag_key = base64.b64encode(
                    tag.key.encode('utf-8')).decode('utf-8')
                chunk[tag.resource_id]['tags'].update({
                    tag_key: tag.value
                })
            migrated_cnt += len(resources)
            resources_collection = _get_resources_collection()
            resources_collection.insert_many(list(chunk.values()))
        if rows != migrated_cnt:
            raise Exception('Some of resources was not migrated')
        _create_unique_index(resources_collection)
        _create_search_indexes(resources_collection)
    except Exception:
        resources_collection.drop()
        raise
    finally:
        session.close()

    op.drop_constraint('assignment_request_ibfk_1', 'assignment_request',
                       type_='foreignkey')
    op.create_index(op.f('ix_assignment_request_resource_id'),
                    'assignment_request', ['resource_id'], unique=False)

    op.drop_constraint('constraint_resource_fk', 'resource_constraint',
                       type_='foreignkey')
    op.drop_index('constraint_resource_fk', table_name='resource_constraint')
    op.create_index(op.f('ix_resource_constraint_resource_id'),
                    'resource_constraint', ['resource_id'], unique=False)

    op.drop_constraint('limit_hit_resource_fk', 'constraint_limit_hit',
                       type_='foreignkey')
    op.drop_index('limit_hit_resource_fk', table_name='constraint_limit_hit')
    op.create_index(op.f('ix_constraint_limit_hit_resource_id'),
                    'constraint_limit_hit', ['resource_id'], unique=False)

    op.drop_constraint(
        'resource_assignment_history_ibfk_2', 'resource_assignment_history',
        type_='foreignkey')
    # when creating a foreign key, we create an index with the same name,
    # but as a result of some manipulations for this field, the index name
    # differs from the foreignkey name. If you delete it and recreate a new one
    # it will be created with the name resource_assignment_history_ibfk_2.
    # This construction is necessary here in order to avoid an error if we
    # upgrade/downgrade call several times
    try:
        op.drop_index('resource_id',
                      table_name='resource_assignment_history')
    except ProgrammingError:
        op.drop_index('resource_assignment_history_ibfk_2',
                      table_name='resource_assignment_history')
    op.create_index(
        op.f('ix_resource_assignment_history_resource_id'),
        'resource_assignment_history', ['resource_id'], unique=False)

    op.drop_table('tag')
    op.drop_table('resource_cache')
    op.drop_table('resource_cache_request')
    op.drop_table('resource')
    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        'resource',
        sa.Column('id', sa.String(length=36), nullable=False),
        sa.Column('deleted_at', sa.Integer(), nullable=False),
        sa.Column('created_at', sa.Integer(), nullable=False),
        sa.Column('cloud_resource_id', sa.String(length=512), nullable=False),
        sa.Column('name', sa.String(length=256), nullable=True),
        sa.Column('region', sa.String(length=256), nullable=True),
        sa.Column('employee_id', sa.String(length=36), nullable=True),
        sa.Column('cloud_account_id', sa.String(length=36),
                  nullable=False),
        sa.Column('budget_id', sa.String(length=36), nullable=True),
        sa.Column('resource_type', sa.String(length=256), nullable=False),
        sa.Column('meta', sa.TEXT(), nullable=True),

        sa.ForeignKeyConstraint(['budget_id'], ['budget.id'], ),
        sa.ForeignKeyConstraint(['cloud_account_id'], ['cloudaccount.id'], ),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('cloud_resource_id', 'cloud_account_id',
                            'deleted_at', name='uc_res_acc_del_at')
    )
    op.create_table(
        'resource_cache_request',
        sa.Column('id', sa.String(length=36), nullable=False),
        sa.Column('valid_until', sa.Integer(), nullable=False),
        sa.Column('resource_type', sa.Enum(
            'instance', 'volume', 'snapshot', 'bucket'), nullable=False),
        sa.Column('business_unit_id', sa.String(length=36), nullable=False),
        sa.Column('meta', sa.TEXT(), nullable=False),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_table(
        'resource_cache',
        sa.Column('id', sa.String(length=36), nullable=False),
        sa.Column('cache_id', sa.String(length=36), nullable=False),
        sa.Column('cloud_resource_id', sa.String(length=512), nullable=True),
        sa.Column('resource_id', sa.String(length=36), nullable=True),
        sa.Column('region', sa.String(length=255), nullable=True),
        sa.Column('cloud_account_id', sa.String(length=36), nullable=True),
        sa.Column('cloud_account_name', sa.String(length=256), nullable=True),

        sa.Column('size', sa.Integer(), nullable=True),
        sa.Column('volume_type', sa.String(length=32), nullable=True),
        sa.Column('name', sa.String(length=256), nullable=True),
        sa.Column('flavor', sa.String(length=64), nullable=True),
        sa.Column('business_unit_id', sa.String(length=256), nullable=False),
        sa.Column('description', sa.TEXT(), nullable=True),
        sa.Column('state', sa.String(length=64), nullable=True),
        sa.ForeignKeyConstraint(['cache_id'], ['resource_cache_request.id'], ),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_table(
        'tag',
        sa.Column('id', sa.String(length=36), nullable=False),
        sa.Column('resource_id', sa.String(36), nullable=False),
        sa.Column('key', sa.String(256), nullable=False),
        sa.Column('value', sa.String(256), nullable=False),
        sa.ForeignKeyConstraint(['resource_id'], ['resource.id'], ),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('id')
    )

    resources_collection = _get_resources_collection()
    try:
        tag_table = table(
            'tag',
            column('id', sa.String(36)),
            column('resource_id', sa.String(36)),
            column('key', sa.String(256)),
            column('value', sa.String(256))
        )
        resource_table = table(
            'resource',
            column('id', sa.String(length=36)),
            column('deleted_at', sa.Integer()),
            column('created_at', sa.Integer()),
            column('cloud_resource_id', sa.String(length=512)),
            column('name', sa.String(length=256)),
            column('region', sa.String(length=256)),
            column('employee_id', sa.String(length=36)),
            column('cloud_account_id', sa.String(length=36)),
            column('budget_id', sa.String(length=36)),
            column('resource_type', sa.String(length=256)),
            column('meta', sa.TEXT())
        )
        rows = resources_collection.count()
        migrated_cnt = 0
        while migrated_cnt < rows:
            resources = resources_collection.aggregate([
                {'$skip': migrated_cnt},
                {'$limit': CHUNK_SIZE}
            ])
            resources = list(resources)
            tags_bulk = []
            for r in resources:
                r.pop('last_seen', None)
                r['id'] = r.pop('_id')
                meta = r.pop('meta', {}) or {}
                tags = r.pop('tags', {}) or {}
                for k, v in tags.items():
                    tag_key = base64.b64decode(
                        k.encode('utf-8')).decode('utf-8')
                    tags_bulk.append({
                        'id': gen_id(), 'resource_id': r['id'],
                        'key': tag_key, 'value': v
                    })
                r['meta'] = json.dumps(meta)
            migrated_cnt += len(resources)
            op.bulk_insert(resource_table, resources)
            op.bulk_insert(tag_table, tags_bulk)
        if rows != migrated_cnt:
            raise Exception('Some of resources were not migrated')
    except Exception:
        op.drop_table('tag')
        op.drop_table('resource_cache')
        op.drop_table('resource_cache_request')
        op.drop_table('resource')
        raise

    op.drop_index(op.f('ix_resource_assignment_history_resource_id'),
                  table_name='resource_assignment_history')
    op.drop_index(op.f('ix_constraint_limit_hit_resource_id'),
                  table_name='constraint_limit_hit')
    op.drop_index(op.f('ix_resource_constraint_resource_id'),
                  table_name='resource_constraint')
    op.drop_index(op.f('ix_assignment_request_resource_id'),
                  table_name='assignment_request')

    op.create_foreign_key('resource_assignment_history_ibfk_2',
                          'resource_assignment_history', 'resource',
                          ['resource_id'], ['id'])
    op.create_foreign_key('limit_hit_resource_fk', 'constraint_limit_hit',
                          'resource', ['resource_id'], ['id'])
    op.create_foreign_key('constraint_resource_fk', 'resource_constraint',
                          'resource', ['resource_id'], ['id'])
    op.create_foreign_key('assignment_request_ibfk_1', 'assignment_request',
                          'resource', ['resource_id'], ['id'])

    resources_collection.drop()
    # ### end Alembic commands ###
